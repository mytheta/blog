# 第三章　確率的勾配降下法
宮下弓槻
佐藤隼明
## 過適合の緩和
### 正則化
過適合とは，学習時の誤差関数の浅い局所解にトラップされた状況であると解釈できる．重みの数が多いほど，そうなる可能性がある．そこで，学習時の重みの自由度を制約する正則化によって，緩和する．

正則化は過学習を抑えるための重要な手法である．
分類にしても回帰にしても，入出力関係y=f(w,x)についてwを上手く調整するのが目的である．この際にwが際限なく自由に値を取れてしまっては過学習してしまう危険性でてくる．

そこで，wに対して何らかの制約を入れてやるのが正則化である．



### 重みの制約
多層ニューラルネットワークなどでは重みが発散しやすいので，よく使われている
誤差関数に重みの二乗和を加算し，これを最小化することで，重みの制約を与える．

$$　E_t(w) \equiv \frac{1}{N_t}\sum_{n\in D_t} E_{n}(w) + \frac{\lambda}{2}\|w\|^2  $$


$\lambda$ はこの正則化の強さを制御するパラメータ．この項の追加により，学習時により小さい重みが選好されるようになる．λ は正則化の強さを制御するパラメータで、0.1〜0.00001程度にするらしい．

勾配降下法の更新式は，

$$ w^{(t＋1)} = w^{(t)} - \varepsilon ( \frac{1}{N_t}\sum \nabla E + \lambda w^{(t)} )$$

となり，重みは，自身の大きさに比例した速さで減衰するように修正される(**重み減衰** と呼ぶ)．

#### 重み上限
各ユニットの入力側結合の，重みの二乗和の上限を制約する方法である．
$I$ 層のユニットｊについて考えると，これが，$I-1$層のユニットi=1,..., $I$ からの出力を受け取るとすると，その間の結合重み
$w_{ji}$ が
$$
\sum_{i = 1} {w_{ji}}^2
  $$

  を満たすように重みを制約する．

#### メリット
* 訓練データに対する過学習を抑制することで,新規のデータに対する汎用性をより持たせることが出来る．
* 無駄な重みを縮小させることにより，隠れ層をより滑らかに，解釈しやすくすることができる．

## ドロップアウト
そもそも，過学習を防ぐ一番簡単な方法は、学習データを増やすことである．  
しかし，適切な学習データをたくさん作ることは大変..  
誤差関数の値をなるべく小さくしたい
はずれ値があるとそれに引っ張られて誤差関数が大きくなってしまう


過学習を防ぐための方法
→学習時に重みの自由度を制約する（正則化）
・重み減衰（重みを小さくして計算する）
・重み上限（重みに上限を付ける）
・ドロップアウト（自由度を強制的に小さくする）

◯ドロップアウト
ユニットを確率的に選別して学習を進めていく．
中間層の各層と入力層のユニットを決まった割合pでランダムに選出し，それら以外を無効化する。
「仮の姿」のネットワークについて最適化する．

ユニットの選出は重みの更新毎に行う．かつ，各層毎に割合pは違っても良い．

![image](https://products.sint.co.jp/hs-fs/hubfs/images/aisia/blog/8-2.png?t=1526370192164&width=1280&name=8-2.png)


## 学習のトリック
学習時の実行することで，汎化性能を向上し，また学習を早く進められる方法がいくつかある．その多くは，厳密な理論に伴わないノウハウのようなもの．

### データの正規化
訓練データが偏りを含む場合，学習の妨げになるため，偏りがなくなるような前処理を行う．データの正規化はもっともポピュラーである．各サンプル $X_n$ に，線形変換を施し，その成分ごとの平均や分散を揃える．

$$　\overline{x} \equiv \frac{\sum_{n = 1}^N x_{ni} }{N}    $$

 で，平均を求める．そして，標準偏差を求める．

$$　\sigma_i \equiv \sqrt{\frac{1}{N}{\sum_{n = 1}^N (x_{ni}-\overline{x}})^2}
$$

α=max{x1,...,xN}

 この $\sigma_i$ を用いて， $x_{ni}$ を $\frac{x_{ni}-\overline{x}}{\sigma_i}$ とする．そうすることで，変換後のサンプルは，各成分の平均が0,分散が1となる．

 つまり，正規化は学習データに対して，値が0〜1の範囲に収まるように加工を施すことである．場合によっては−1〜1にすることもある(正負両方の値を取るような信号を学習させたい場合など）
 しかし，
多次元データに対してこのような処理が必ず有効であるかは分からない．

なぜなら、学習データxの各成分の大きさを均等に扱っていいかわからないから．画像であれば，すべてピクセルの値（濃淡）であって，対等に扱えるデータだが，
一方で各次元が(身長，体重，体脂肪率)などになっていた場合は，成分毎に意味合いが全く異なるため，各成分毎に正規化をする（身長は身長だけで正規化する）ことが必要になるかもしれない．

### データの拡張
訓練データの少ないときに用いる．ようするに「水増し」．手持ちのサンプルデータに加工を施し，量を増やす．

### 複数ネットの平均
複数の異なるニューラルネットを組み合わせると，一般に推定精度を向上できる，学習および，テスト時の計量が増えてしますのが欠点．

### 学習係数の決め方
手動(試行錯誤しながら)が一般的．定番の考え方が二つ
学習係数が大きいと最小値に達しても、それを飛び越えてしまう危険がある(下図左)。学習係数が小さいと、極小値で停留してしまう危険があります(下図右)。
![image](https://cdn-ak.f.st-hatena.com/images/fotolife/Y/Yaju3D/20170925/20170925010945.png)

#### 一つ目
学習の初期ほど大きな値を選び，学習の進捗とともに学習係数をちいさくする．

#### 二つ目
ネットワークの全ての層で，同じ学習係数を用いるのではなく,層ごとに異なる値を使うこと．出力の浅い層では，学習係数を小さく，入力に近い深層では大きくするらしい．

### モメンタム
勾配降下法の収束性能を向上させる方法の一つ．重みの修正量に，前回の重みの修正量の幾ばくかを加算する方法．誤差関数が深い谷状の形状を持ち，かつその谷底にあまり高低差がないときに用いる．
#### わかりやすく言うと
丘からボールが落ちるのと同じで，転がり落ちる際に勾配の方向に加速して行く．  
今ボールが落ちている方向(慣性)と，勾配方向が同じなら加速，逆なら原則という感じの更新方法．

### サンプルの順序
確率的勾配降下法を使う場合，訓練サンプルをどの順番で取り出すかには，自由度がある．一般的には，ネットワークが「見慣れない」サンプルを先に提示すると学習が最も効率的に進む．
