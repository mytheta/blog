---
layout: single
title: 確率的勾配降下法
categories: 機械学習
tags:
 - パターン認識
 - 機械学習
paginate: true
image: /assets/images/hakone.jpg
toc: true
toc_label: "My Table of Contents"
author_profile: true
---
## 誤差関数
機械学習アルゴリズムにおいて，どれだけ学習できているかということを判断するための指標を，「誤差関数」という．ニューラルネットワークの場合は，ネットワークが表す関数と訓練データの近さの尺度のこと．
## 勾配降下法

$$  訓練データ:D = \{ (x_1,x_d,...,x_N,d_N) \} $$

勾配降下法とは，上の訓練データを元に計算される誤差関数   $$E(w)$$ を，ネットワークパラメータ(重み)$$w$$ について最小化することである．．多クラス分類での誤差関数は，

$$ E(w) = -\sum_{n=1}^N \sum_{k=1}^K d_nk\log y_k(x_n;w)
 $$

 この出力値を最小にすれば，誤差が小さくなるということであり，よい．よって．wの最小値を求める．勾配降下法の勾配とは，

 $$ \nabla E \equiv \frac{\partial E}{\partial w} = \left[
    \frac{\partial E}{\partial w_1}... \frac{\partial E}{\partial w_M}
  \right]^T $$

というベクトルである(Mはwの成分数)．勾配降下法は，現在の重みを $$w^{(t)}$$,動かした重みを,$$w^{(t＋1)}$$ とすると

$$ w^{(t＋1)} = w^{(t)} - \varepsilon \nabla E $$

の式で，表すことができ,初期値 $$w^{(1)}$$ 適当に決め,t=1,2,3,...に対し繰り返し計算することで，$$w^{(2)},w^{(3)},...$$ を得る( $$\varepsilon$$ はwの更新量の大きさを定める定数)．そして，tを大きく，つまり何度も更新を繰り返すことで，極小点へ到達する．

## 確率的勾配降下法
サンプルの一部を使って，パラメータの更新を行う方法．wの更新は，一つのサンプルnについて計算される誤差関数 $$E_n(w)の勾配 \nabla E_n$$ を計算し，


$$ w^{(t＋1)} = w^{(t)} - \varepsilon \nabla E_n $$


のようにwを更新する．次の $$w^{(t+1)}$$ の更新の際は，別のサンプル $$n'$$ を取り出し，これを使って同様に勾配 $$\nabla E_{n'}$$ を評価し，wを更新する，このようにサンプルを毎回取り替え，ｗを更新していく．

### メリット
* 訓練データに冗長性がある場合，計算効率が向上し学習が速く実行できる．
* サンプルの水増しの影響は学習内容・計算量ともにない．
* 反復計算が望まない局所的な極小解にトラップされてしまうリスクの低減(w
の更新ごとに異なる　$$\nabla E_{n}(w)$$ であるため)．

## バッチ学習
全訓練サンプルn=1,2,3,...Nに対して計算される誤差関数用いて学習を行う方法．

## ミニバッチ学習
全体を考慮したバッチ学習と、確率的勾配法の間を取った学習．重みの更新をサンプル一つ単位で行うのではなく，少数のサンプルの集合をひとまとめにし，その単位で重みを更新する．そのひとまとめにしたサンプルの集合をミニバッチと呼ぶ( $$D_t$$ )．なので，$$D_t$$ の含む全サンプルに対する誤差

$$ E_t(w) = 1/N\sum_{n\in D_t} E_{n}(w)
 $$

を計算し，その勾配の方向にパラメータを更新する．

## 学習の本当の目的
さきほどまで，訓練データに対する誤差を最小化することを考えてきた．学習の本当の目的は，与えられた訓練データではなくて，これから与えられるはずの「まだ見ぬ」サンプルxに対して正しい空いてを行うこと．

### 訓練誤差
訓練データに対する誤差
### 汎化誤差
サンプルの母集団に対する誤差の期待値

訓練誤差と汎化誤差が乖離した状態のことを，過適合，過剰適合，過学習という．

## 過適合の緩和
### 正則化
過適合とは，学習時の誤差関数の浅い局所解にトラップされた状況であると解釈できる．重みの数が多いほど，そうなる可能性がある．そこで，学習時の重みの自由度を制約する正則化によって，緩和する．

### 重みの制約
誤差関数に重みの二乗和を加算し，これを最小化することで，重みの制約を与える．

$$　E_t(w) \equiv \frac{1}{N_t}\sum_{n\in D_t} E_{n}(w) + \frac{\lambda}{2}\|w\|^2  $$

$$\lambda$$ はこの正則化の強さを制御するパラメータ．この項の追加により，学習時により小さい重みが選好されるようになる．勾配降下法の更新式は，

$$ w^{(t＋1)} = w^{(t)} - \varepsilon ( \frac{1}{N_t}\sum \nabla E + \lambda w^{(t)} )$$

となり，重みは，自身の大きさに比例した速さで減衰するように修正される(重み減衰と呼ぶ)．

## ドロップアウト
ドロップアウトは，多層ネットワークのユニットを確率的に選別して学習する方法．ニューラルネットワークを学習する際に、ある更新で層の中のノードのうちのいくつかを無効にして（そもそも存在しないかのように扱って）学習を行い、次の更新では別のノードを無効にして学習を行うことを繰り返します。これにより学習時にネットワークの自由度を強制的に小さくして汎化性能を上げ、過学習を避けることができる．

## 学習のトリック
学習時の実行することで，汎化性能を向上し，また学習を早く進められる方法がいくつかある．その多くは，厳密な理論に伴わないノウハウのようなもの．

### データの正規化
訓練データが偏りを含む場合，学習の妨げになるため，偏りがなくなるような前処理を行う．データの正規化はもっともポピュー．各サンプル $X_n$ に，線形変換を施し，その成分ごとの平均や分散を揃える．

### データの拡張
訓練データの少ないときに用いる．ようするに「水増し」．手持ちのサンプルデータに加工を施し，量を増やす．

### 複数ネットの平均
複数の異なるニューラルネットを組み合わせると，一般に推定精度を向上できる，学習および，テスト時の計量が増えてしますのが欠点．

### 学習係数の決め方
手動(試行錯誤しながら)が一般的．定番の考え方が二つ

#### 一つ目
学習の初期ほど大きな値を選び，学習の進捗とともに学習係数をちいさくする．

#### 二つ目
ネットワークの全ての層で，同じ学習係数を用いるのではなく,層ごとに異なる値を使うこと．出力の浅い層では，学習係数を小さく，入力に近い深層では大きくするらしい．

### モメンタム
勾配降下法の収束性能を向上させる方法の一つ．重みの修正量に，前回の重みの修正量の幾ばくかを加算する方法．誤差関数が深い谷状の形状を持ち，かつその谷底にあまり高低差がないときに用いる．

### サンプルの順序
確率的勾配降下法を使う場合，訓練サンプルをどの順番で取り出すかには，自由度がある．一般的には，ネットワークが「見慣れない」サンプルを先に提示すると学習が最も効率的に進む．
