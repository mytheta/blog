---
layout: single
title: 確率的勾配降下法
categories: 機械学習
tags:
 - パターン認識
 - 機械学習
paginate: true
image: /assets/images/hakone.jpg
toc: true
toc_label: "My Table of Contents"
author_profile: true
---
## 誤差関数
機械学習アルゴリズムにおいて，どれだけ学習できているかということを判断するための指標を，「誤差関数」という．ニューラルネットワークの場合は，ネットワークが表す関数と訓練データの近さの尺度のこと．

## 勾配とは？
勾配(gradient)とはスカラー場φに対して、φがもっととも大きく変化する方向を向き、その変化量と同じ大きさを持つベクトルのことで，
$\nabla E$　で表す．
例えば，二次元の勾配はこのように表せられる．
$$\nabla f=(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}) $$
fについて具体的に考えてみる．
$$ f(x,y) = 3x + 4y$$
とするなら，
$$ \frac{\partial f}{\partial x} = 3,  \frac{\partial f}{\partial y} = 4$$
つまり，
$$\nabla f=(3,4)$$
この値が何なのかを考えてみる．
![image](http://hooktail.sub.jp/mathInPhys/vecFuntou1/yakan-grad-fig1.png)
![image](http://hooktail.sub.jp/mathInPhys/vecFuntou1/yakan-grad-fig2.png)

$\nabla f$ は x の変化率と  y の変化率を方向も含めて合成した，一番変化率の高い（坂で言えば勾配のきつい）方向を向いてるベクトルということがわかる．
これは，二次関数の場合だが，機械学習では，fは高次元のものを扱う．

![image](https://cdn-ak.f.st-hatena.com/images/fotolife/Y/Yaju3D/20170924/20170924183956.png)

## 勾配降下法
さて，勾配について理解したとこで，勾配降下法についてみていきたいと思います．
### 勾配降下法の目的
誤差関数の最小となるwを見つけたい．
### アプローチ
変化量の大きい勾配を辿っていき，極小点を求める．

$$  訓練データ:D = \{ (x_1,d_1),...,(x_N,d_N) \} $$

上の訓練データを元に計算される誤差関数(交差エントロピー)    $E(w)$ は,

$$ E(w) = -\sum_{n=1}^N \sum_{k=1}^K d_nk\log y_k(x_n;w)
 $$


 この出力値を最小にすれば，誤差が小さくなるということであり，よい．

 また，勾配降下法の勾配とは，

 $$ \nabla E \equiv \frac{\partial E}{\partial w} = \left[
    \frac{\partial E}{\partial w_1}... \frac{\partial E}{\partial w_M}
  \right]^T $$

というベクトルである(Mはwの成分数)．勾配降下法は，現在の重みを $w^{(t)}$,動かした重みを,$w^{(t＋1)}$ とすると

$$ w^{(t＋1)} = w^{(t)} - \varepsilon \nabla E $$

の式で，表すことができる．これは，現在位置 $w^{(t)}$ での勾配を計算し，その勾配に学習率 $\varepsilon$ を掛けたもので位置を更新している．
 初期値 $w^{(1)}$ 適当に決め,t=1,2,3,...に対し繰り返し計算することで，$w^{(2)},w^{(3)},...$ を得る( $\varepsilon$ はwの更新量の大きさを定める定数)．そして，tを大きく，つまり何度も更新を繰り返すことで，極小点へ到達する．

 ![image](https://camo.qiitausercontent.com/8d967c1d622b7a4d95062cd65de33cccf67ecbef/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3134373038352f36656236613465652d393735652d633431652d643033342d6462643162323566643465652e706e67)

## 確率的勾配降下法
では，次に確率的勾配降下法についてみてみる．
確率的勾配降下法は，サンプルの一部を使って，パラメータの更新を行う方法．重みの更新の時，一つの訓練サンプルを用いる．
## なぜ確率的勾配降下法なのか
深層学習において，真の最小値を見つけるのは難しい．しかし，誤差関数の値があまりに大きい臨界点(微分係数がすべて0の点)にハマってしまえば，使い物にならない．そこで，臨界点にトラップされることを回避するためにランダムな要素を取り入れて，ハマり混んだ場所から弾き出すため，確率的勾配降下法を用いる．

### 計算
wの更新は，一つのサンプルnについて計算される誤差関数 $E_n(w)の勾配 \nabla E_n$ を計算し，

$$ w^{(t＋1)} = w^{(t)} - \varepsilon \nabla E_n $$


のようにwを更新する．次の $w^{(t+1)}$ の更新の際は，別のサンプル $n'$ を取り出し，これを使って同様に勾配 $\nabla E_{n'}$ を評価し，wを更新する，このようにサンプルを毎回取り替え，ｗを更新していく．

## バッチ学習
全訓練サンプルn=1,2,3,...Nに対して計算される誤差関数用いて学習を行う方法．

## ミニバッチ学習
全体を考慮したバッチ学習と、確率的勾配法の間を取った学習．重みの更新をサンプル一つ単位で行うのではなく，少数のサンプルの集合をひとまとめにし，その単位で重みを更新する．そのひとまとめにしたサンプルの集合をミニバッチと呼ぶ( $D_t$ )．なので，$D_t$ の含む全サンプルに対する誤差

$$ E_t(w) = 1/N\sum_{n\in D_t} E_{n}(w)
 $$

を計算し，その勾配の方向にパラメータを更新する．

### なぜミニバッチ学習なのか
ミニバッチ学習では，重みの更新ごとに誤差関数の形もランダムに変化する．したがって，望ましくない臨界点にハマりこむ可能性が少なくなる．さらに，効率的にサンプルを使用することができる（似たデータによる重複学習を防げる）．


## 学習の本当の目的
さきほどまで，訓練データに対する誤差を最小化することを考えてきた．学習の本当の目的は，与えられた訓練データではなくて，これから与えられるはずの「まだ見ぬ」サンプルxに対して正しい推定を行うこと．

### 訓練誤差
訓練データに対する誤差

### 汎化誤差
サンプルの母集団に対する誤差の期待値

### 過適合
一番左の状態は，訓練誤差の値が大きすぎて，これでは何の予測能力も得られない．このような状況は，アンダーフィッティングや未学習と呼ばれる．  
また，一番右の状態は，与えられた訓練データに関する訓練に適合しすぎて，訓練データではない未知のデータに対して予測能力を失うこと．このような状態を過学習，過剰適合という．
我々が見つけたい汎化の状況は，これらの状態の中間である，真ん中のグラフである．
![image](https://cdn-ssl-devio-img.classmethod.jp/wp-content/uploads/2015/05/mlconcepts_image51.png)

## 過適合の緩和
### 正則化
過適合とは，学習時の誤差関数の浅い局所解にトラップされた状況であると解釈できる．重みの数が多いほど，そうなる可能性がある．そこで，学習時の重みの自由度を制約する正則化によって，緩和する．

正則化は過学習を抑えるための重要な手法である．
分類にしても回帰にしても，入出力関係y=f(w,x)についてwを上手く調整するのが目的である．この際にwが際限なく自由に値を取れてしまっては過学習してしまう危険性でてくる．

そこで，wに対して何らかの制約を入れてやるのが正則化である．



### 重みの制約
多層ニューラルネットワークなどでは重みが発散しやすいので，よく使われている
誤差関数に重みの二乗和を加算し，これを最小化することで，重みの制約を与える．

$$　E_t(w) \equiv \frac{1}{N_t}\sum_{n\in D_t} E_{n}(w) + \frac{\lambda}{2}\|w\|^2  $$


$\lambda$ はこの正則化の強さを制御するパラメータ．この項の追加により，学習時により小さい重みが選好されるようになる．λ は正則化の強さを制御するパラメータで、0.1〜0.00001程度にするらしい．

勾配降下法の更新式は，

$$ w^{(t＋1)} = w^{(t)} - \varepsilon ( \frac{1}{N_t}\sum \nabla E + \lambda w^{(t)} )$$

となり，重みは，自身の大きさに比例した速さで減衰するように修正される(**重み減衰** と呼ぶ)．

#### 重み上限
各ユニットの入力側結合の，重みの二乗和の上限を制約する方法である．
$I$ 層のユニットｊについて考えると，これが，$I-1$層のユニットi=1,..., $I$ からの出力を受け取るとすると，その間の結合重み
$w_{ji}$ が
$$
\sum_{i = 1} {w_{ji}}^2　＜　ｃ
  $$

  を満たすように重みを制約する．重みが大きくならないように，ある定数cより重みが大きくなってしまったら重みに1以下の定数を掛け，満たすようにする．

#### メリット
* 訓練データに対する過学習を抑制することで,新規のデータに対する汎用性をより持たせることが出来る．
* 無駄な重みを縮小させることにより，隠れ層をより滑らかに，解釈しやすくすることができる．

## ドロップアウト
機械学習において，予測性能をあげるには，複数のニューラルネットに学習させるのが望ましい．クラス分類ならすべての結果の多数決を取り，回帰なら出力の平均を採用する．
しかし，複数のニューラルネットを用いることは，とても計算コストがかさみ，現実的ではない．
そこで，編み出された手法がドロップアウトである．
しかし，必ずしも過学習を防ぐ訳ではない．

過学習を防ぐための方法
→学習時に重みの自由度を制約する（正則化）
・重み減衰（重みを小さくして計算する）
・重み上限（重みに上限を付ける）
・ドロップアウト（自由度を強制的に小さくする）

◯ドロップアウト
ユニットを確率的に選別して学習を進めていく．
中間層の各層と入力層のユニットを決まった割合pでランダムに選出し，それら以外を無効化する。
「仮の姿」のネットワークについて最適化する．

ユニットの選出は重みの更新毎に行う．かつ，各層毎に割合pは違っても良い．

![image](https://products.sint.co.jp/hs-fs/hubfs/images/aisia/blog/8-2.png?t=1526370192164&width=1280&name=8-2.png)


## 学習のトリック
学習時の実行することで，汎化性能を向上し，また学習を早く進められる方法がいくつかある．その多くは，厳密な理論に伴わないノウハウのようなもの．

### データの正規化
訓練データが偏りを含む場合，学習の妨げになるため，偏りがなくなるような前処理を行う．データの正規化はもっともポピュラーである．各サンプル $X_n$ に，線形変換を施し，その成分ごとの平均や分散を揃える．

$$　\overline{x} \equiv \frac{\sum_{n = 1}^N x_{ni} }{N}    $$

 で，平均を求める．そして，標準偏差を求める．

$$　\sigma_i \equiv \sqrt{\frac{1}{N}{\sum_{n = 1}^N (x_{ni}-\overline{x}})^2}
$$

α=max{x1,...,xN}

 この $\sigma_i$ を用いて， $x_{ni}$ を $\frac{x_{ni}-\overline{x}}{\sigma_i}$ とする．そうすることで，変換後のサンプルは，各成分の平均が0,分散が1となる．

 つまり，正規化は学習データに対して，値が0〜1の範囲に収まるように加工を施すことである．場合によっては−1〜1にすることもある(正負両方の値を取るような信号を学習させたい場合など）
 しかし，
多次元データに対してこのような処理が必ず有効であるかは分からない．

なぜなら、学習データxの各成分の大きさを均等に扱っていいかわからないから．画像であれば，すべてピクセルの値（濃淡）であって，対等に扱えるデータだが，
一方で各次元が(身長，体重，体脂肪率)などになっていた場合は，成分毎に意味合いが全く異なるため，各成分毎に正規化をする（身長は身長だけで正規化する）ことが必要になるかもしれない．

### データの拡張
訓練データの少ないときに用いる．たくさんの訓練データを用意するのはとてもコストがかかる．しかし，データ数が少ないと過学習を起こしてしまう．そこで，手持ちのサンプルから擬似データを作成することを考える．
画像認識において，画像の回転や並進，歪みを加える操作でも，写っている対象の本質は損なわれない．画像が少し歪んだだけで機会学習で識別できないということは困るので，あえて手を加えた画像を訓練データに加えて水増しする．このような作業をデータの拡張という．

### 複数ネットの平均
複数の異なるニューラルネットを組み合わせると，一般に推定精度を向上できる，学習および，テスト時の計量が増えてしますのが欠点．

### 学習係数の決め方
手動(試行錯誤しながら)が一般的．定番の考え方が二つ
学習係数が大きいと最小値に達しても、それを飛び越えてしまう危険がある(下図左)。学習係数が小さいと、極小値で停留してしまう危険があります(下図右)。
![image](https://cdn-ak.f.st-hatena.com/images/fotolife/Y/Yaju3D/20170925/20170925010945.png)

#### 一つ目
学習の初期ほど大きな値を選び，学習の進捗とともに学習係数をちいさくする．

#### 二つ目
ネットワークの全ての層で，同じ学習係数を用いるのではなく,層ごとに異なる値を使うこと．出力の浅い層では，学習係数を小さく，入力に近い深層では大きくするらしい．

### モメンタム
勾配降下法の収束性能を向上させる方法の一つ．重みの修正量に，前回の重みの修正量の幾ばくかを加算する方法．誤差関数が深い谷状の形状を持ち，かつその谷底にあまり高低差がないときに用いる．
#### わかりやすく言うと
丘からボールが落ちるのと同じで，転がり落ちる際に勾配の方向に加速して行く．  
今ボールが落ちている方向(慣性)と，勾配方向が同じなら加速，逆なら原則という感じの更新方法．

### サンプルの順序
確率的勾配降下法を使う場合，訓練サンプルをどの順番で取り出すかには，自由度がある．一般的には，ネットワークが「見慣れない」サンプルを先に提示すると学習が最も効率的に進む．
